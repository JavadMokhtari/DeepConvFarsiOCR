{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-bzXmNn0mmw"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jOk_9tBR8De-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwlabX975qYD"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constant parameters\n",
    "IMG_WIDTH = 30\n",
    "IMG_HEIGHT = 30\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56529 images belonging to 36 classes.\n",
      "Found 14116 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create an Image Generator\n",
    "img_gen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2)\n",
    "\n",
    "# Create train dataset with ImageGenerator from local directory\n",
    "train_ds = img_gen.flow_from_directory(\n",
    "    directory='Train-Data/',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='grayscale',\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    interpolation='bilinear',\n",
    "    batch_size=48)\n",
    "\n",
    "# Create validation dataset with ImageGenerator from local directory\n",
    "val_ds = img_gen.flow_from_directory(\n",
    "    directory='Train-Data/',\n",
    "    subset='validation',\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='grayscale',\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    interpolation='bilinear',\n",
    "    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of one batch of data:\n",
      " (48, 30, 30, 1) \n",
      "\n",
      "Shape of one batch of labels:\n",
      " (48, 36) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Shape of one batch of data:\\n', train_ds[0][0].shape, '\\n')\n",
    "print('Shape of one batch of labels:\\n', train_ds[0][1].shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f071073d520>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show one sample of data\n",
    "img = train_ds[0][0][4]\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVo8iPCe0z_9"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "ENg1Ukhx07vx"
   },
   "outputs": [],
   "source": [
    "# Define CNN + FullyConnectedNetwork Model\n",
    "OCR_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, 5, input_shape=(30, 30, 1)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Conv2D(128, 5),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(250),\n",
    "    keras.layers.ReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(36),\n",
    "    keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 64)        1664      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 128)         204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 250)               512250    \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 250)               1000      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 36)                9036      \n",
      "_________________________________________________________________\n",
      "softmax_2 (Softmax)          (None, 36)                0         \n",
      "=================================================================\n",
      "Total params: 730,158\n",
      "Trainable params: 729,018\n",
      "Non-trainable params: 1,140\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "OCR_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "OCR_model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2YAWEMr09m1"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178/1178 [==============================] - 877s 741ms/step - loss: 0.6810 - accuracy: 0.7932 - val_loss: 0.2684 - val_accuracy: 0.9139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f06c018b130>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model to train data\n",
    "OCR_model.fit(train_ds,\n",
    "    epochs=20,\n",
    "    validation_data=val_ds,\n",
    "    # Early Stopping prevents from overfitting\n",
    "    callbacks=keras.callbacks.EarlyStopping(patience=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsVznLyR1Azu"
   },
   "source": [
    "## Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "WeLzJMd61FyX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17706 images belonging to 36 classes.\n",
      "369/369 [==============================] - 3s 9ms/step - loss: 0.2835 - accuracy: 0.9126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2834565341472626, 0.9126284718513489]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create test dataset from images saved in 'Test-Data' folder\n",
    "img_gen_ = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_ds = img_gen_.flow_from_directory(\n",
    "    directory='Test-Data/',\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='grayscale',\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    interpolation='bilinear',\n",
    "    batch_size=48)\n",
    "\n",
    "# Evaluation accuracy of model\n",
    "OCR_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.998, 0.   , 0.   , 0.001, 0.   , 0.   , 0.   ,\n",
       "        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.001, 0.   , 0.   ,\n",
       "        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
       "        0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict one sample of data\n",
    "OCR_model.predict(test_ds[0][0][0].reshape((1, 30, 30, 1))).round(3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa085a72276727e76d8d3f8b72696a7c658025f3bc1cfc2762ce514c7f33be68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
